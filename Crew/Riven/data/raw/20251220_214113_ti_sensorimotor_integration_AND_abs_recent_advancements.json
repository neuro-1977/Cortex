[
  {
    "title": "SFTok: Bridging the Performance Gap in Discrete Tokenizers",
    "summary": "Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).",
    "published": "2025-12-18T18:59:04Z",
    "link": "http://arxiv.org/abs/2512.16910v1"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
    "published": "2025-12-18T18:59:03Z",
    "link": "http://arxiv.org/abs/2512.16909v1"
  },
  {
    "title": "A survey of the orienteering problem: model evolution, algorithmic advances, and future directions",
    "summary": "The orienteering problem (OP) is a combinatorial optimization problem that seeks a path visiting a subset of locations to maximize collected rewards under a limited resource budget. This article presents a systematic PRISMA-based review of OP research published between 2017 and 2025, with a focus on models and methods that have shaped subsequent developments in the field. We introduce a component-based taxonomy that decomposes OP variants into time-, path-, node-, structure-, and information-based extensions. This framework unifies classical and emerging variants -- including stochastic, time-dependent, Dubins, Set, and multi-period OPs -- within a single structural perspective. We further categorize solution approaches into exact algorithms, heuristics and metaheuristics, and learning-based methods, with particular emphasis on matheuristics and recent advances in artificial intelligence, especially reinforcement learning and neural networks, which enhance scalability in large-scale and information-rich settings. Building on this unified view, we discuss how different components affect computational complexity and polyhedral properties and identify open challenges related to robustness, sustainability, and AI integration. The survey thus provides both a consolidated reference for existing OP research and a structured agenda for future theoretical and applied work.",
    "published": "2025-12-18T18:35:33Z",
    "link": "http://arxiv.org/abs/2512.16865v1"
  },
  {
    "title": "DenseBEV: Transforming BEV Grid Cells into 3D Objects",
    "summary": "In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.",
    "published": "2025-12-18T17:59:22Z",
    "link": "http://arxiv.org/abs/2512.16818v1"
  },
  {
    "title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories",
    "summary": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.",
    "published": "2025-12-18T17:18:45Z",
    "link": "http://arxiv.org/abs/2512.16784v1"
  },
  {
    "title": "Kling-Omni Technical Report",
    "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
    "published": "2025-12-18T17:08:12Z",
    "link": "http://arxiv.org/abs/2512.16776v1"
  },
  {
    "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
    "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
    "published": "2025-12-18T16:57:44Z",
    "link": "http://arxiv.org/abs/2512.16760v1"
  },
  {
    "title": "Spin-Dependent Nonorthogonal Generalized Wannier Functions and their Integration with PAW and Hubbard Corrections in Linear-Scaling DFT",
    "summary": "We present a spin-dependent extension of the non-orthogonal generalized Wannier function (NGWF) formalism within the framework of linear-scaling density functional theory (LS-DFT) as implemented in the ONETEP code. In traditional LS-DFT representations, both spin channels are constrained to share a common variational basis, which limits the accuracy for systems that are spin-polarized or exhibit magnetic order. Our approach allows NGWFs to vary independently for each spin channel, enabling a more accurate representation of spin-polarization in the electronic density. We demonstrate the efficacy of this method through a series of test cases, including localized magnetic defects in two-dimensional hBN, transition metal complexes, two-dimensional van der Waals magnetic materials, and both bulk and nanocluster ferromagnetic Co. In each scenario, the incorporation of spin-dependent NGWFs results in enhanced accuracy for total energy calculations, improved localization of spin density, and accurate predictions of magnetic ground states. This improvement is particularly notable when combined with DFT+U and DFT+U+J corrections. In this work, we take the opportunity to describe the combination of DFT+U+J and the projector-augmented wave (PAW) formalism within the LS-DFT framework, including how PAW participates in the ionic Pulay force, and in the minimum-tracking linear response approach for computing parameters in situ. Our findings demonstrate that spin-dependent NGWFs are a crucial and computationally efficient advancement in the linear-scaling DFT simulation of spin-polarized materials.",
    "published": "2025-12-18T16:40:25Z",
    "link": "http://arxiv.org/abs/2512.16744v1"
  },
  {
    "title": "TreeNet: A Light Weight Model for Low Bitrate Image Compression",
    "summary": "Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.",
    "published": "2025-12-18T16:40:06Z",
    "link": "http://arxiv.org/abs/2512.16743v1"
  },
  {
    "title": "Deep learning directed synthesis of fluid ferroelectric materials",
    "summary": "Fluid ferroelectrics, a recently discovered class of liquid crystals that exhibit switchable, long-range polar order, offer opportunities in ultrafast electro-optic technologies, responsive soft matter, and next-generation energy materials. Yet their discovery has relied almost entirely on intuition and chance, limiting progress in the field. Here we develop and experimentally validate a deep-learning data-to-molecule pipeline that enables the targeted design and synthesis of new organic fluid ferroelectrics. We curate a comprehensive dataset of all known longitudinally polar liquid-crystal materials and train graph neural networks that predict ferroelectric behaviour with up to 95% accuracy and achieve root mean square errors as low as 11 K for transition temperatures. A graph variational autoencoder generates de novo molecular structures which are filtered using an ensemble of high-performing classifiers and regressors to identify candidates with predicted ferroelectric nematic behaviour and accessible transition temperatures. Integration with a computational retrosynthesis engine and a digitised chemical inventory further narrows the design space to a synthesis-ready longlist. 11 candidates were synthesised and characterized through established mixture-based extrapolation methods. From which extrapolated ferroelectric nematic transitions were compared against neural network predictions. The experimental verification of novel materials augments the original dataset with quality feedback data thus aiding future research. These results demonstrate a practical, closed-loop approach to discovering synthesizable fluid ferroelectrics, marking a step toward autonomous design of functional soft materials.",
    "published": "2025-12-18T15:41:29Z",
    "link": "http://arxiv.org/abs/2512.16671v1"
  }
]